â¸»

âš™ï¸ COME SI DEFINISCONO GLI IPERPARAMETRI DI DIFFUSIONNET

DiffusionNet non Ã¨ una CNN tradizionale â€” i suoi iperparametri si dividono in due categorie:
	1.	ğŸ”¹ Geometrici â†’ determinano la risoluzione e la qualitÃ  della rappresentazione della forma
	2.	ğŸ”¸ Architetturali â†’ determinano la capacitÃ  di apprendimento e la dimensione dellâ€™embedding

Vediamoli uno per uno ğŸ‘‡

â¸»

ğŸ”¹ 1. IPERPARAMETRI GEOMETRICI

ğŸ§® k_eig â€” Numero di autovettori del Laplaciano

Cosâ€™Ã¨

Ãˆ il numero di autofunzioni del Laplaciano discreto che vengono calcolate per costruire la base spettrale.

DiffusionNet sfrutta questi autovettori per rappresentare la geometria come una â€œFourier basisâ€ sul dominio della mesh.
In pratica: piÃ¹ k_eig â‡’ piÃ¹ componenti ad alta frequenza â‡’ piÃ¹ dettagli geometrici catturati.

Effetti

Valore	Effetto	Quando usarlo
16â€“32	Molto veloce ma molto â€œsmoothâ€	per prototipi su point cloud
64â€“128	Bilanciato	per mesh organiche (es. volti, corpi)
256+	Alta risoluzione ma costoso	per modelli complessi o training multi-shape

Formula empirica (funziona bene)

k_{\text{eig}} \approx \sqrt{N_{\text{verts}}}
Esempio: una mesh da 6890 vertici â†’ sqrt(6890) â‰ˆ 83 â†’ tipicamente scegli k_eig=128.

â¸»

âš–ï¸ normalize_positions

Non Ã¨ un iperparametro, ma Ã¨ una normalizzazione essenziale:
	â€¢	centra i vertici sul baricentro
	â€¢	ridimensiona la mesh per avere raggio unitario

Serve per rendere gli autovalori del Laplaciano confrontabili tra mesh di dimensioni diverse.

ğŸ’¡ Sempre attivo, a meno che tu non voglia studiare effetti di scala (rarissimo).

â¸»

ğŸ§© compute_operators(..., cache_dir=...)

Puoi precomputare e salvare i Laplaciani, autovettori, ecc., in una cache.
Questo Ã¨ un iperparametro tecnico:
	â€¢	utile quando lavori su dataset grandi
	â€¢	irrilevante su singole mesh

â¸»

ğŸ”¸ 2. IPERPARAMETRI ARCHITETTURALI

ğŸ§± C_in â€” Numero di feature in input

Ogni vertice puÃ² avere:
	â€¢	coordinate (x, y, z) â†’ 3 feature (default)
	â€¢	normali â†’ +3 feature
	â€¢	HKS o WKS descriptor â†’ +16â€“128 feature

ğŸ’¡ In pratica:

Caso	C_in
solo coordinate	3
coordinate + normali	6
coordinate + HKS(16)	19
embedding multi-modal	64+


â¸»

ğŸŒŠ C_width â€” Larghezza del canale interno

Ãˆ lâ€™equivalente del numero di feature per layer in una CNN.

Effetti:

Valore	Impatto	Uso tipico
32	leggero e veloce	debug o GPU piccola
64	bilanciato	shape reconstruction
128	solido	face embedding o segmentation
256+	costoso	multi-shape learning o multimodale

ğŸ’¡ Aumentare C_width quadruplica la memoria perchÃ© ogni layer elabora [B, V, C_width].

â¸»

ğŸ¯ C_out â€” Numero di feature in output

Dipende dal tipo di task:

Task	C_out tipico	Significato
Embedding non supervisionato	32â€“128	vettore descrittivo del vertice
Segmentazione per-vertex	#classi (es. 10)	probabilitÃ  di classe
Face encoder (tipo autoencoder)	64â€“256	embedding compatto della geometria

ğŸ’¡ Se non addestri, ma vuoi solo ottenere un embedding, scegli C_out=64 o 128.

â¸»

ğŸ”¥ last_activation

Attivazione finale â€” serve per adattare lâ€™output al tipo di loss.

Attivazione	Uso
None	per embedding â€œgrezziâ€
torch.nn.functional.log_softmax	per classificazione (CrossEntropyLoss)
torch.tanh	per normalizzare lâ€™embedding (es. cosine similarity)
torch.sigmoid	per task binari o mask


â¸»

ğŸ§  outputs_at

Definisce a quale entitÃ  topologica associare lâ€™output:
	â€¢	'vertices' â†’ classico per embedding facciali, segmentation, ecc.
	â€¢	'faces' â†’ utile per materiali o campi scalari per faccia
	â€¢	'edges' â†’ per task tipo orientamento o deformazione (raro)

â¸»

âš—ï¸ 3. IPERPARAMETRI DI TRAINING (secondari)

Parametro	Default	Significato
dropout	False	aggiungi rumore per regolarizzare
N_block	4	numero di blocchi DiffusionNet impilati
mlp_hidden_dim	C_width	dimensione dello spazio MLP dopo la diffusione
batch_norm	True	normalizzazione inter-feature

ğŸ’¡ Aumentare N_block migliora la capacitÃ  della rete, ma la complessitÃ  cresce linearmente.

â¸»

ğŸ’¡ 4. COME SCEGLIERLI (strategie pratiche)

ğŸ”¬ Se fai embedding di volti (6890 vertici)

K_EIG = 128
C_WIDTH = 128
C_OUT = 64
N_BLOCK = 4
dropout = False

âš™ï¸ Se fai shape segmentation (10 classi)

K_EIG = 64
C_WIDTH = 128
C_OUT = 10
last_activation = log_softmax

âš¡ Se vuoi solo testare la rete

K_EIG = 32
C_WIDTH = 64
C_OUT = 8


â¸»

ğŸ§® 5. Regole di scaling

Una buona euristica (che puoi anche scrivere in tesi):

QuantitÃ 	Scala consigliata
k_eig	â‰ˆ âˆš(n_verts)
C_width	2 Ã— logâ‚‚(k_eig) Ã— 16
C_out	â‰¤ C_width
learning_rate	1e-3 (per training)
batch_size	4â€“8 (dipende da GPU)


â¸»

ğŸ§© 6. Riassunto concettuale

 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚        INPUT: vertices (x,y,z) âˆˆ â„Â³                      â”‚
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â”‚ normalize_positions()                                    â”‚
 â”‚ compute_operators(k_eig)  â†’ L, evals, evecs, gradX/Y     â”‚
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â”‚ DiffusionNet(C_in, C_width, C_out)                       â”‚
 â”‚     â””â”€â”€ N_block times diffusion + MLP                    â”‚
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â”‚ OUTPUT: per-vertex embedding (â„^{C_out})                 â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



Descrizione
1.
Allena una DiffusionNet su BFM (C_in=3, C_out=64) con triplet loss tra soggetti
2.
Allena unâ€™altra DiffusionNet su FLAME con stesso setup
3.
Calcola embedding globali per 100 soggetti su entrambe le topologie
4.
Calcola similaritÃ  coseno intra-soggetto e inter-soggetto
5.
Visualizza in t-SNE / PCA la separazione tra identitÃ 

Da quello che dici:

â€œmi serve solo una cosa che crea un oggetto a partire da un altroâ€

tu non vuoi una rete discriminativa o classificatrice,
ma un encoder che proietti le mesh in uno spazio geometrico coerente â€”
in modo che mesh simili â†’ embedding vicini.

ğŸ‘‰ Quindi sÃ¬: serve un training, ma non di classificazione.
Serve un training auto-supervisionato o contrastivo
perchÃ© la rete deve imparare cosa significa â€œessere similiâ€ geometricamente.

Senza questo, lâ€™output di DiffusionNet Ã¨ solo una trasformazione casuale dei vertici.

Strategia
Descrizione
Dati richiesti
Unsupervised / Autoencoder
la rete ricostruisce la mesh dal proprio embedding â†’ impara a codificare forma
solo mesh
Contrastive (SimCLR-style)
due versioni augmentate della stessa mesh devono avere embedding simili
solo mesh (senza label)
Triplet / Cosine Loss supervisionato
due mesh stesso soggetto vicine, diverse lontane
gruppi per soggetto (che tu hai!)
WBES-guided training (ibrido)
la rete minimizza la distanza embedding per forme a basso errore WBES
risultati FaceBench

ğŸ” 4ï¸âƒ£ Variante piÃ¹ â€œambiziosaâ€ (e piÃ¹ complessa)

Puoi anche cercare di condividere parzialmente i pesi tra i due encoder:
	â€¢	stesso backbone (DiffusionNet layers),
	â€¢	ma operatori geometrici diversi (L, gradX, gradY).

CosÃ¬ provi a â€œforzareâ€ il modello ad imparare una rappresentazione comune tra topologie â€”
una sorta di shared latent manifold.

Ãˆ lâ€™idea dietro il cross-topology shape embedding:

due reti diverse (una per topologia) â†’ embedding nello stesso spazio metrico.

Concetto
Spiegazione
Due encoder diversi â‰  stesse coordinate
ognuno impara il proprio sistema di riferimento
Câ€™Ã¨ offset o rotazione globale
sÃ¬, sempre; non Ã¨ un problema
CiÃ² che conta Ã¨ la coerenza locale
embedding simili â†’ soggetti simili
Si puÃ² correggere a posteriori
via Procrustes o regressione lineare
Obiettivo finale
ottenere embedding topologicamente compatibili, non identici


