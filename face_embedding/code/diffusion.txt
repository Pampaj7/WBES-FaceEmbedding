⸻

⚙️ COME SI DEFINISCONO GLI IPERPARAMETRI DI DIFFUSIONNET

DiffusionNet non è una CNN tradizionale — i suoi iperparametri si dividono in due categorie:
	1.	🔹 Geometrici → determinano la risoluzione e la qualità della rappresentazione della forma
	2.	🔸 Architetturali → determinano la capacità di apprendimento e la dimensione dell’embedding

Vediamoli uno per uno 👇

⸻

🔹 1. IPERPARAMETRI GEOMETRICI

🧮 k_eig — Numero di autovettori del Laplaciano

Cos’è

È il numero di autofunzioni del Laplaciano discreto che vengono calcolate per costruire la base spettrale.

DiffusionNet sfrutta questi autovettori per rappresentare la geometria come una “Fourier basis” sul dominio della mesh.
In pratica: più k_eig ⇒ più componenti ad alta frequenza ⇒ più dettagli geometrici catturati.

Effetti

Valore	Effetto	Quando usarlo
16–32	Molto veloce ma molto “smooth”	per prototipi su point cloud
64–128	Bilanciato	per mesh organiche (es. volti, corpi)
256+	Alta risoluzione ma costoso	per modelli complessi o training multi-shape

Formula empirica (funziona bene)

k_{\text{eig}} \approx \sqrt{N_{\text{verts}}}
Esempio: una mesh da 6890 vertici → sqrt(6890) ≈ 83 → tipicamente scegli k_eig=128.

⸻

⚖️ normalize_positions

Non è un iperparametro, ma è una normalizzazione essenziale:
	•	centra i vertici sul baricentro
	•	ridimensiona la mesh per avere raggio unitario

Serve per rendere gli autovalori del Laplaciano confrontabili tra mesh di dimensioni diverse.

💡 Sempre attivo, a meno che tu non voglia studiare effetti di scala (rarissimo).

⸻

🧩 compute_operators(..., cache_dir=...)

Puoi precomputare e salvare i Laplaciani, autovettori, ecc., in una cache.
Questo è un iperparametro tecnico:
	•	utile quando lavori su dataset grandi
	•	irrilevante su singole mesh

⸻

🔸 2. IPERPARAMETRI ARCHITETTURALI

🧱 C_in — Numero di feature in input

Ogni vertice può avere:
	•	coordinate (x, y, z) → 3 feature (default)
	•	normali → +3 feature
	•	HKS o WKS descriptor → +16–128 feature

💡 In pratica:

Caso	C_in
solo coordinate	3
coordinate + normali	6
coordinate + HKS(16)	19
embedding multi-modal	64+


⸻

🌊 C_width — Larghezza del canale interno

È l’equivalente del numero di feature per layer in una CNN.

Effetti:

Valore	Impatto	Uso tipico
32	leggero e veloce	debug o GPU piccola
64	bilanciato	shape reconstruction
128	solido	face embedding o segmentation
256+	costoso	multi-shape learning o multimodale

💡 Aumentare C_width quadruplica la memoria perché ogni layer elabora [B, V, C_width].

⸻

🎯 C_out — Numero di feature in output

Dipende dal tipo di task:

Task	C_out tipico	Significato
Embedding non supervisionato	32–128	vettore descrittivo del vertice
Segmentazione per-vertex	#classi (es. 10)	probabilità di classe
Face encoder (tipo autoencoder)	64–256	embedding compatto della geometria

💡 Se non addestri, ma vuoi solo ottenere un embedding, scegli C_out=64 o 128.

⸻

🔥 last_activation

Attivazione finale — serve per adattare l’output al tipo di loss.

Attivazione	Uso
None	per embedding “grezzi”
torch.nn.functional.log_softmax	per classificazione (CrossEntropyLoss)
torch.tanh	per normalizzare l’embedding (es. cosine similarity)
torch.sigmoid	per task binari o mask


⸻

🧠 outputs_at

Definisce a quale entità topologica associare l’output:
	•	'vertices' → classico per embedding facciali, segmentation, ecc.
	•	'faces' → utile per materiali o campi scalari per faccia
	•	'edges' → per task tipo orientamento o deformazione (raro)

⸻

⚗️ 3. IPERPARAMETRI DI TRAINING (secondari)

Parametro	Default	Significato
dropout	False	aggiungi rumore per regolarizzare
N_block	4	numero di blocchi DiffusionNet impilati
mlp_hidden_dim	C_width	dimensione dello spazio MLP dopo la diffusione
batch_norm	True	normalizzazione inter-feature

💡 Aumentare N_block migliora la capacità della rete, ma la complessità cresce linearmente.

⸻

💡 4. COME SCEGLIERLI (strategie pratiche)

🔬 Se fai embedding di volti (6890 vertici)

K_EIG = 128
C_WIDTH = 128
C_OUT = 64
N_BLOCK = 4
dropout = False

⚙️ Se fai shape segmentation (10 classi)

K_EIG = 64
C_WIDTH = 128
C_OUT = 10
last_activation = log_softmax

⚡ Se vuoi solo testare la rete

K_EIG = 32
C_WIDTH = 64
C_OUT = 8


⸻

🧮 5. Regole di scaling

Una buona euristica (che puoi anche scrivere in tesi):

Quantità	Scala consigliata
k_eig	≈ √(n_verts)
C_width	2 × log₂(k_eig) × 16
C_out	≤ C_width
learning_rate	1e-3 (per training)
batch_size	4–8 (dipende da GPU)


struct?

cross_topo/
│
├── config.py                # parametri globali e setup GPU
│
├── dataset/
│   ├── __init__.py
│   ├── mesh_dataset.py      # gestione dataset per singola topologia
│   ├── sampler.py           # accoppiamento BFM-FLAME e negative sampling
│   └── operators_cache.py   # caching di Laplaciani e operatori geometrici
│
├── models/
│   ├── __init__.py
│   ├── backbone.py          # DiffusionBackbone condivisa
│   └── losses.py            # triplet_loss, cosine_distance, ecc.
│
├── train/
│   ├── __init__.py
│   └── train_cross_topo.py  # main training loop (usa le parti sopra)
│
└── utils/
    ├── parsing.py           # parse_subject_id, logging, seed
    └── eval_utils.py        # t-SNE, PCA, plotting


🧠 1. Il principio generale: metric / contrastive learning

La parte di Triplet loss + embedding normalizzati è uno standard consolidato nel metric learning.
È la stessa idea usata in:

FaceNet (Schroff et al., CVPR 2015)

ArcFace, CosFace, SimCLR, ecc.

Solo che lì si applica su immagini 2D, mentre tu la stai adattando a forme 3D.
L’obiettivo è identico:

avvicinare embedding di esempi “positivi” e allontanare quelli “negativi”.

🧩 2. Il contesto 3D: cross-domain / cross-topology embedding

Nel 3D questo tipo di setup si usa per allineare rappresentazioni di domini diversi,
esattamente come tu stai facendo tra topologie diverse (BFM, FLAME, ecc.).

Ecco esempi noti in letteratura:

Paper	Idea chiave	Analogia con il tuo setup
“Shape–DCL: Shape Contrastive Learning” (Zeng et al., CVPR 2021)	usa triplet/contrastive loss su shape embedding 3D	stesso tipo di loss e filosofia
“Cross-domain Shape Matching with Contrastive Learning” (Zhao et al., 2022)	allinea shape da domini diversi (es. scan vs mesh)	uguale al tuo caso: BFM ↔ FLAME
“Unsupervised Cross-modal 3D Shape Retrieval” (Han et al., 2020)	allinea feature da rappresentazioni diverse	analogo a topologie diverse
“Learning Consistent Shape Embeddings Across Topologies” (You et al., 2023)	specificamente per cross-topology embedding	esattamente quello che stai cercando di fare

👉 Quindi sì: l’idea del shared backbone + triplet + alignment loss è documentata.
La tua implementazione è una versione pulita e generalizzata di quello schema.

⚙️ 3. Cosa stai realmente facendo

Tu hai formalizzato la cosa come:

BFM ↔ FLAME
↓        ↓
same encoder (shared weights)
↓        ↓
embedding space ℝᵈ
↓
Triplet + MSE losses


Questo schema è matematicamente coerente con la teoria dell’invarianza per embedding geometrici.

È una combinazione tra:

alignment loss (come nei domain adaptation)

triplet loss (come nei metric learning setups)

È quindi un cross-domain metric learning setup — una classe di metodi esistenti e molto studiati.

1. Compute DiffusionNet operators for each mesh (L, evecs, gradX/Y, etc.)
2. Feed them into a shared DiffusionNet backbone → get embeddings
3. Pair meshes of the same subject across topologies (BFM ↔ FLAME)
4. Train with Triplet + Alignment loss to enforce:
      same subject → similar embedding
      different subject → dissimilar embedding
