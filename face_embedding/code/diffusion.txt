â¸»

âš™ï¸ COME SI DEFINISCONO GLI IPERPARAMETRI DI DIFFUSIONNET

DiffusionNet non Ã¨ una CNN tradizionale â€” i suoi iperparametri si dividono in due categorie:
	1.	ğŸ”¹ Geometrici â†’ determinano la risoluzione e la qualitÃ  della rappresentazione della forma
	2.	ğŸ”¸ Architetturali â†’ determinano la capacitÃ  di apprendimento e la dimensione dellâ€™embedding

Vediamoli uno per uno ğŸ‘‡

â¸»

ğŸ”¹ 1. IPERPARAMETRI GEOMETRICI

ğŸ§® k_eig â€” Numero di autovettori del Laplaciano

Cosâ€™Ã¨

Ãˆ il numero di autofunzioni del Laplaciano discreto che vengono calcolate per costruire la base spettrale.

DiffusionNet sfrutta questi autovettori per rappresentare la geometria come una â€œFourier basisâ€ sul dominio della mesh.
In pratica: piÃ¹ k_eig â‡’ piÃ¹ componenti ad alta frequenza â‡’ piÃ¹ dettagli geometrici catturati.

Effetti

Valore	Effetto	Quando usarlo
16â€“32	Molto veloce ma molto â€œsmoothâ€	per prototipi su point cloud
64â€“128	Bilanciato	per mesh organiche (es. volti, corpi)
256+	Alta risoluzione ma costoso	per modelli complessi o training multi-shape

Formula empirica (funziona bene)

k_{\text{eig}} \approx \sqrt{N_{\text{verts}}}
Esempio: una mesh da 6890 vertici â†’ sqrt(6890) â‰ˆ 83 â†’ tipicamente scegli k_eig=128.

â¸»

âš–ï¸ normalize_positions

Non Ã¨ un iperparametro, ma Ã¨ una normalizzazione essenziale:
	â€¢	centra i vertici sul baricentro
	â€¢	ridimensiona la mesh per avere raggio unitario

Serve per rendere gli autovalori del Laplaciano confrontabili tra mesh di dimensioni diverse.

ğŸ’¡ Sempre attivo, a meno che tu non voglia studiare effetti di scala (rarissimo).

â¸»

ğŸ§© compute_operators(..., cache_dir=...)

Puoi precomputare e salvare i Laplaciani, autovettori, ecc., in una cache.
Questo Ã¨ un iperparametro tecnico:
	â€¢	utile quando lavori su dataset grandi
	â€¢	irrilevante su singole mesh

â¸»

ğŸ”¸ 2. IPERPARAMETRI ARCHITETTURALI

ğŸ§± C_in â€” Numero di feature in input

Ogni vertice puÃ² avere:
	â€¢	coordinate (x, y, z) â†’ 3 feature (default)
	â€¢	normali â†’ +3 feature
	â€¢	HKS o WKS descriptor â†’ +16â€“128 feature

ğŸ’¡ In pratica:

Caso	C_in
solo coordinate	3
coordinate + normali	6
coordinate + HKS(16)	19
embedding multi-modal	64+


â¸»

ğŸŒŠ C_width â€” Larghezza del canale interno

Ãˆ lâ€™equivalente del numero di feature per layer in una CNN.

Effetti:

Valore	Impatto	Uso tipico
32	leggero e veloce	debug o GPU piccola
64	bilanciato	shape reconstruction
128	solido	face embedding o segmentation
256+	costoso	multi-shape learning o multimodale

ğŸ’¡ Aumentare C_width quadruplica la memoria perchÃ© ogni layer elabora [B, V, C_width].

â¸»

ğŸ¯ C_out â€” Numero di feature in output

Dipende dal tipo di task:

Task	C_out tipico	Significato
Embedding non supervisionato	32â€“128	vettore descrittivo del vertice
Segmentazione per-vertex	#classi (es. 10)	probabilitÃ  di classe
Face encoder (tipo autoencoder)	64â€“256	embedding compatto della geometria

ğŸ’¡ Se non addestri, ma vuoi solo ottenere un embedding, scegli C_out=64 o 128.

â¸»

ğŸ”¥ last_activation

Attivazione finale â€” serve per adattare lâ€™output al tipo di loss.

Attivazione	Uso
None	per embedding â€œgrezziâ€
torch.nn.functional.log_softmax	per classificazione (CrossEntropyLoss)
torch.tanh	per normalizzare lâ€™embedding (es. cosine similarity)
torch.sigmoid	per task binari o mask


â¸»

ğŸ§  outputs_at

Definisce a quale entitÃ  topologica associare lâ€™output:
	â€¢	'vertices' â†’ classico per embedding facciali, segmentation, ecc.
	â€¢	'faces' â†’ utile per materiali o campi scalari per faccia
	â€¢	'edges' â†’ per task tipo orientamento o deformazione (raro)

â¸»

âš—ï¸ 3. IPERPARAMETRI DI TRAINING (secondari)

Parametro	Default	Significato
dropout	False	aggiungi rumore per regolarizzare
N_block	4	numero di blocchi DiffusionNet impilati
mlp_hidden_dim	C_width	dimensione dello spazio MLP dopo la diffusione
batch_norm	True	normalizzazione inter-feature

ğŸ’¡ Aumentare N_block migliora la capacitÃ  della rete, ma la complessitÃ  cresce linearmente.

â¸»

ğŸ’¡ 4. COME SCEGLIERLI (strategie pratiche)

ğŸ”¬ Se fai embedding di volti (6890 vertici)

K_EIG = 128
C_WIDTH = 128
C_OUT = 64
N_BLOCK = 4
dropout = False

âš™ï¸ Se fai shape segmentation (10 classi)

K_EIG = 64
C_WIDTH = 128
C_OUT = 10
last_activation = log_softmax

âš¡ Se vuoi solo testare la rete

K_EIG = 32
C_WIDTH = 64
C_OUT = 8


â¸»

ğŸ§® 5. Regole di scaling

Una buona euristica (che puoi anche scrivere in tesi):

QuantitÃ 	Scala consigliata
k_eig	â‰ˆ âˆš(n_verts)
C_width	2 Ã— logâ‚‚(k_eig) Ã— 16
C_out	â‰¤ C_width
learning_rate	1e-3 (per training)
batch_size	4â€“8 (dipende da GPU)


struct?

cross_topo/
â”‚
â”œâ”€â”€ config.py                # parametri globali e setup GPU
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mesh_dataset.py      # gestione dataset per singola topologia
â”‚   â”œâ”€â”€ sampler.py           # accoppiamento BFM-FLAME e negative sampling
â”‚   â””â”€â”€ operators_cache.py   # caching di Laplaciani e operatori geometrici
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ backbone.py          # DiffusionBackbone condivisa
â”‚   â””â”€â”€ losses.py            # triplet_loss, cosine_distance, ecc.
â”‚
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ train_cross_topo.py  # main training loop (usa le parti sopra)
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ parsing.py           # parse_subject_id, logging, seed
    â””â”€â”€ eval_utils.py        # t-SNE, PCA, plotting


ğŸ§  1. Il principio generale: metric / contrastive learning

La parte di Triplet loss + embedding normalizzati Ã¨ uno standard consolidato nel metric learning.
Ãˆ la stessa idea usata in:

FaceNet (Schroff et al., CVPR 2015)

ArcFace, CosFace, SimCLR, ecc.

Solo che lÃ¬ si applica su immagini 2D, mentre tu la stai adattando a forme 3D.
Lâ€™obiettivo Ã¨ identico:

avvicinare embedding di esempi â€œpositiviâ€ e allontanare quelli â€œnegativiâ€.

ğŸ§© 2. Il contesto 3D: cross-domain / cross-topology embedding

Nel 3D questo tipo di setup si usa per allineare rappresentazioni di domini diversi,
esattamente come tu stai facendo tra topologie diverse (BFM, FLAME, ecc.).

Ecco esempi noti in letteratura:

Paper	Idea chiave	Analogia con il tuo setup
â€œShapeâ€“DCL: Shape Contrastive Learningâ€ (Zeng et al., CVPR 2021)	usa triplet/contrastive loss su shape embedding 3D	stesso tipo di loss e filosofia
â€œCross-domain Shape Matching with Contrastive Learningâ€ (Zhao et al., 2022)	allinea shape da domini diversi (es. scan vs mesh)	uguale al tuo caso: BFM â†” FLAME
â€œUnsupervised Cross-modal 3D Shape Retrievalâ€ (Han et al., 2020)	allinea feature da rappresentazioni diverse	analogo a topologie diverse
â€œLearning Consistent Shape Embeddings Across Topologiesâ€ (You et al., 2023)	specificamente per cross-topology embedding	esattamente quello che stai cercando di fare

ğŸ‘‰ Quindi sÃ¬: lâ€™idea del shared backbone + triplet + alignment loss Ã¨ documentata.
La tua implementazione Ã¨ una versione pulita e generalizzata di quello schema.

âš™ï¸ 3. Cosa stai realmente facendo

Tu hai formalizzato la cosa come:

BFM â†” FLAME
â†“        â†“
same encoder (shared weights)
â†“        â†“
embedding space â„áµˆ
â†“
Triplet + MSE losses


Questo schema Ã¨ matematicamente coerente con la teoria dellâ€™invarianza per embedding geometrici.

Ãˆ una combinazione tra:

alignment loss (come nei domain adaptation)

triplet loss (come nei metric learning setups)

Ãˆ quindi un cross-domain metric learning setup â€” una classe di metodi esistenti e molto studiati.

1. Compute DiffusionNet operators for each mesh (L, evecs, gradX/Y, etc.)
2. Feed them into a shared DiffusionNet backbone â†’ get embeddings
3. Pair meshes of the same subject across topologies (BFM â†” FLAME)
4. Train with Triplet + Alignment loss to enforce:
      same subject â†’ similar embedding
      different subject â†’ dissimilar embedding
